{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() #load all the enviroment variables\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "#Langsmith Tracking\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") \n",
    "os.environ[\"LANGSMITH_TRACING\"]= \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]= \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)\n",
    "'''\n",
    "from langchain_ollama import OllamaLLM\n",
    "llm=OllamaLLM(model=\"gemma:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input and get response from LLM\n",
    "\n",
    "result= llm.invoke(\"What is generative AI?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the definition of generative AI:\n",
      "\n",
      "**Generative AI** is a type of artificial intelligence that can create new content, such as text, images, videos, and music. Unlike **traditional AI**, which is designed to perform specific tasks, generative AI is focused on generating new things rather than understanding existing information.\n",
      "\n",
      "**Here are some key characteristics of generative AI:**\n",
      "\n",
      "* **Creativity:** Generative AI can create novel and original content that is not present in the training data.\n",
      "* **Self-supervised learning:** Generative AI can learn from unlabeled data, meaning it does not need explicit instructions to create new content.\n",
      "* **Continuous generation:** Generative AI can generate content in real-time, allowing for continuous creation of new outputs.\n",
      "* **Large scale:** Generative AI models can be much larger than traditional AI models, with trillions of parameters.\n",
      "\n",
      "**Examples of generative AI models include:**\n",
      "\n",
      "* **ChatGPT**\n",
      "* **DALL-E 2**\n",
      "* **Generative Adversarial Networks (GANs)**\n",
      "* **Variational Autoencoders (VAEs)**\n",
      "\n",
      "Generative AI has a wide range of applications, including:\n",
      "\n",
      "* **Content creation:** Creating marketing materials, news articles, and entertainment content.\n",
      "* **Image and video editing:** Editing existing images and videos to enhance their quality.\n",
      "* **Drug discovery:** Identifying new drug candidates and designing new drugs.\n",
      "* **Music composition:** Creating new musical pieces and songs.\n",
      "\n",
      "Generative AI is still a relatively new field, but it is rapidly evolving and making significant advancements.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Tempelate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a summary of Langsmith:\n",
      "\n",
      "**Langsmith** is a **natural language processing (NLP) model** that specializes in **semantic search**. It is developed by **OpenAI** and is trained on a massive dataset of text and code.\n",
      "\n",
      "**Key features of Langsmith:**\n",
      "\n",
      "* **Semantic search:** It allows users to ask questions about the content of a piece of text, and the model will find similar and related text based on meaning rather than just keywords.\n",
      "* **Multilingual:** It supports multiple languages, making it useful for tasks such as cross-lingual question answering and document retrieval.\n",
      "* **Long-form text processing:** It can handle long pieces of text, unlike traditional NLP models that are limited to shorter texts.\n",
      "* **Multimodal:** It can process and understand text alongside other modalities such as images and videos.\n",
      "\n",
      "**How Langsmith works:**\n",
      "\n",
      "1. **Text representation:** The text is converted into a numerical representation using a multi-modal transformer.\n",
      "2. **Attention mechanisms:** The model focuses on the most relevant parts of the text based on the query.\n",
      "3. **Ranking:** The model ranks results based on their semantic similarity to the query.\n",
      "\n",
      "**Use cases for Langsmith:**\n",
      "\n",
      "* **Question answering:** Ask questions about a piece of text and get related information.\n",
      "* **Text retrieval:** Find similar and relevant text based on a query.\n",
      "* **Document search:** Use keywords and natural language queries to find relevant documents.\n",
      "* **Multilingual language processing:** Translate text between different languages.\n",
      "\n",
      "**Additional information:**\n",
      "\n",
      "* Langsmith is in **beta testing** and is not yet widely available.\n",
      "* It is trained on a massive dataset of text and code, which means it is very well-informed.\n",
      "* The model is designed to be **robust** and can handle a wide range of tasks.\n",
      "\n",
      "I hope this summary is helpful. Please let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "## Chain\n",
    "chain= prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\": \"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you learn about Langsmith.\n",
      "\n",
      "**Langsmith** is an open-source natural language processing (NLP) library and toolkit built for the Python programming language. It is designed to be **highly performant** and efficient, making it suitable for various NLP tasks, including:\n",
      "\n",
      "* Text classification\n",
      "* Sentiment analysis\n",
      "* Named entity recognition\n",
      "* Text summarization\n",
      "* Machine translation\n",
      "* Question answering\n",
      "\n",
      "**Key features of Langsmith include:**\n",
      "\n",
      "* **High performance:** Langsmith is designed to be faster than other NLP libraries, such as NLTK.\n",
      "* **Large language model support:** It includes support for a wide range of pre-trained language models, including BERT, GPT, and RoBERTa.\n",
      "* **Easy to use:** Langsmith is designed to be simple and easy to use, with a clear and well-documented API.\n",
      "* **Extensive documentation:** The official documentation provides comprehensive tutorials, examples, and reference materials.\n",
      "* **Active community:** Langsmith has a large and active community of developers and users who contribute to its development and provide support.\n",
      "\n",
      "**Use cases for Langsmith include:**\n",
      "\n",
      "* **Data science projects:** Langsmith can be used to perform various data science tasks, such as sentiment analysis and text classification.\n",
      "* **Web development:** It can be used to build chatbots, machine translation systems, and other web applications.\n",
      "* **Natural language interfaces:** Langsmith can be used to create user-friendly natural language interfaces for various devices.\n",
      "* **Research and education:** Langsmith can be used in research and education projects to perform various NLP tasks.\n",
      "\n",
      "**Additional resources:**\n",
      "\n",
      "* Official website: [langsmith.org](langsmith.org)\n",
      "* Documentation: [langsmith.org/docs](langsmith.org/docs)\n",
      "* GitHub repository: [github.com/langsmith-ai/langsmith](github.com/langsmith-ai/langsmith)\n",
      "* Active community forum: [langsmith.org/forum](langsmith.org/forum)\n"
     ]
    }
   ],
   "source": [
    "## String output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "Chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\": \"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Gen AI APP Using Langchain And Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() #load all the enviroment variables\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "#Langsmith Tracking\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") \n",
    "os.environ[\"LANGSMITH_TRACING\"]= \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]= \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1e27a353850>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader= WebBaseLoader(\"https://docs.smith.langchain.com/tutorials/Administrators/manage_spend\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': '🦜️🛠️ LangSmith', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could not find what you were looking for.Head back to our main docs page or use the search bar to find the page you need.CommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs= loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Divide Data into Chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents= text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': '🦜️🛠️ LangSmith', 'language': 'en'}, page_content='🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could not find what you were looking for.Head back to our main docs page or use the search bar to find the page you need.CommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain_openai import OpenAIEmbeddings\\nembeddings=OpenAIEmbeddings()\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Vector Embedding\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings= (\n",
    "    OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    ")\n",
    "'''\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1e27a352aa0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could not find what you were looking for.Head back to our main docs page or use the search bar to find the page you need.CommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"LangSmith has two usage limits: total traces and extended\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain_openai import ChatOpenAI\\nllm=ChatOpenAI(model=\"gpt-4o\")\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "llm=OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "'''\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context::\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "| OllamaLLM(model='gemma:2b')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrival Chain, Document Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt= ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context::\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, I can answer this question based on the context you provided:\\n\\nAccording to the context, LangSmith has two usage limits: total traces and extended traces.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\": \"LangSmith has two usage limits: total traces and extended\",\n",
    "    \"context\":[Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. \")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1e27a352aa0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input---->Retriever---->vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import  create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001E27A352AA0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context::\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "            | OllamaLLM(model='gemma:2b')\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am unable to answer this question based on the context. I am unable to access external sources or provide information outside of the context provided.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response from the LLM\n",
    "response=retrieval_chain.invoke({\"input\": \"LangSmith has two usage limits: total traces and extended\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
